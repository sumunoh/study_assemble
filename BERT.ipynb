{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "_14 BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumunoh/study_assemble/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttMLpgHe0qS0"
      },
      "source": [
        "# BERT(Bidirectional Encoder Representations from Transformers)\n",
        "\n",
        "* 참고: https://ebbnflow.tistory.com/151\n",
        "* 참고: https://github.com/NLP-kr/tensorflow-ml-nlp-tf2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9UVaxPPBXup"
      },
      "source": [
        "## Input Representation\n",
        "\n",
        "* 3가지의 입력 임베딩(Token, Segment, Position 임베딩)의 합으로 구성\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbABsUL%2FbtqzmTU7OLm%2FYwK6JLhNfTYvxkiFzkfkCK%2Fimg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BZpgYyoBlqW"
      },
      "source": [
        "### Token Embeddings\n",
        "\n",
        "* Word Piece 임베딩 방식 사용\n",
        "* 자주 등장하면서 가장 긴 길이의 sub-word를 하나의 단위로 생성\n",
        "* 즉, 자주 등장하는 sub-word은 그 자체가 단위가 되고, 자주 등장하지 않는 단어(rare word)는 sub-word로 쪼개짐\n",
        "* 기존 워드 임베딩 방법은 Out-of-vocabulary (OOV) 문제가 존재하며, 희귀 단어, 이름, 숫자나 단어장에 없는 단어에 대한 학습, 번역에 어려움이 있음\n",
        "* Word Piece 임베딩은 모든 언어에 적용 가능하며, sub-word 단위로 단어를 분절하므로 OOV 처리에 효과적이고 정확도 상승효과도 있음\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_Xa5v1iCBzE"
      },
      "source": [
        "### Sentence Embeddings\n",
        "\n",
        "* BERT는 두 개의 문장을 문장 구분자([SEP])와 함께 결합\n",
        "* 입력 길이의 제한으로 두 문장은 합쳐서 512 subword 이하로 제한\n",
        "* 입력의 길이가 길어질수록 학습시간은 제곱으로 증가하기 때문에 적절한 입력 길이 설정 필요\n",
        "* 한국어는 보통 평균 20 subword로 구성되고 99%가 60 subword를 넘지 않기 때문에 입력 길이를 두 문장이 합쳐 128로 해도 충분\n",
        "* 간혹 긴 문장이 있으므로 우선 입력 길이 128로 제한하고 학습한 후, 128보다 긴 입력들을 모아 마지막에 따로 추가 학습하는 방식을 사용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dloBbq6CD7v"
      },
      "source": [
        "### Position Embedding\n",
        "\n",
        "* BERT는 저자의 이전 논문인 Transformer 모델을 착용\n",
        "* Transformer은 주로 사용하는 CNN, RNN 모델을 사용하지 않고 Self-Attention 모델을 사용\n",
        "* Self-Attention은 입력의 위치에 대해 고려하지 못하므로 입력 토큰의 위치 정보가 필요\n",
        "* Transformer 에서는 Sinusoid 함수를 이용한 Positional encoding을 사용하였고, BERT에서는 이를 변형하여 Position encoding을 사용\n",
        "* Position encoding은 단순하게 Token 순서대로 0, 1, 2, ...와 같이 순서대로 인코딩\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF-JatE-CF7P"
      },
      "source": [
        "### 임베딩 취합\n",
        "\n",
        "* BERT는 위에서 소개한 3가지의 입력 임베딩(Token, Segment, Position 임베딩)을 취합하여 하나의 임베딩 값으로 생성\n",
        "* 임베딩의 합에 Layer Normalization과 Dropout을 적용하여 입력으로 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWYtaq0qCq6P"
      },
      "source": [
        "## 언어 모델링 구조(Pre-training BERT)\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbg5SlP%2FbtqzntBU7Uj%2FKHWiKI4zKgb8FqLzAYAusK%2Fimg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qVY1sl5C8dD"
      },
      "source": [
        "### 언어 모델링 데이터\n",
        "\n",
        "* BERT는 총 3.3억 단어(8억 단어의 BookCorpus 데이터와 25억 단어의 Wikipedia 데이터)의 거대한 말뭉치를 이용하여 학습\n",
        "* 거대한 말뭉치를 MLM, NSP 모델 적용을 위해 스스로 라벨을 만들고 수행하므로 준지도학습(Semi-supervised)이라고 함\n",
        "* Wikipedia와 BookCorpus를 정제하기 위해 list, table, header를 제거\n",
        "* 문장의 순서를 고려해야 하므로 문단 단위로 분리하였고 많은 데이터 정제 작업을 수행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTASWRoyDHKy"
      },
      "source": [
        "### 모델 구조\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbL28Ok%2FbtqznO6UmYw%2Fe0mFyA814Pvj4kltVxKls0%2Fimg.png)\n",
        "\n",
        "\n",
        "* BERT 모델은 Transformer를 기반으로 함\n",
        "* Transformer 모델 구조는 인코더-디코더 모델이며 번역 도메인에서 최고 성능을 기록\n",
        "* 기존 인코더-디코더 모델들과 다르게 Transformer는 CNN, RNN을 이용하지 않고 Self-attention이라는 개념을 도입\n",
        "* BERT는 Transformer의 인코더-디코더 중 인코더만 사용하는 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq2CX1-sDZWz"
      },
      "source": [
        "### MLM(Masked Language Model)\n",
        "\n",
        "* 입력 문장에서 임의로 Token을 마스킹(masking), 그 Token을 맞추는 방식인 MLM 학습 진행\n",
        "* 문장의 빈칸 채우기 문제를 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkKUJCOJEDIN"
      },
      "source": [
        "* 생성 모델 계열은(예를들어 GPT) 입력의 다음 단어를 예측\n",
        "* MLM은 문장 내 랜덤한 단어를 마스킹 하고 이를 예측\n",
        "* 입력의 15% 단어를 [MASK] Token으로 바꿔주어 마스킹\n",
        "* 이 때 80%는 [MASK]로 바꿔주지만, 나머지 10%는 다른 랜덤 단어로, 또 남은 10%는 바꾸지 않고 그대로 둠\n",
        "* 이는 미세 조정 시 올바른 예측을 돕도록 마스킹에 노이즈를 섞음\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FLMyXN%2Fbtqzl4Ql7sH%2FykzRZNWkc6rcb8ffU5Nrm1%2Fimg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYxZ3AV5EMt5"
      },
      "source": [
        "* 아래 그림은 MLM의 학습 과정\n",
        "* 입력 단어의 15%가 [MASK]로 대체된 입력이 들어가고, MLM은 [MASK]가 어떤 단어인지를 예측\n",
        "* BERT의 Token 임베딩은 Word Piece 임베딩 방식을 사용하고, Word piece의 단어수는 30522 단어\n",
        "* 3만 단어 중 [MASK]에 들어갈 단어를 찾는 것이므로 MLM의 출력인 Softmax의 클래스는 3만개\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fc0lfDS%2FbtqzmTOp4JK%2FXkDq157Mw7MnycHeC2NAx1%2Fimg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbIouOAAEW3P"
      },
      "source": [
        "### NSP(Next Sentence Prediction)\n",
        "\n",
        "* NSP는 두 문장이 주어졌을 때 두 번째 문장이 첫 번째 문장의 바로 다음에 오는 문장인지 여부를 예측하는 방식\n",
        "* 두 문장 간 관련이 고려되어야 하는 NLI와 QA의 파인튜닝을 위해 두 문장이 연관이 있는지를 맞추도록 학습\n",
        "* 아래 그림은 NSP의 입력 예시\n",
        "* 위에서 설명한 MLM과 동시에 NSP도 적용된 문장들\n",
        "* 첫 번째 문장과 두 번째 문장은 [SEP]로 구분\n",
        "* 두 문장이 실제로 연속하는지는 50% 비율로 참인 문장과, 50%의 랜덤하게 추출된 상관 없는 문장으로 구성\n",
        "* 이 학습을 통해 문맥과 순서를 언어모델이 학습 가능\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FmRPzz%2Fbtqzps28Eyd%2F2ak5AHBLlk1jXHnOgGwyMK%2Fimg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o2iHcTxEpPf"
      },
      "source": [
        "* 아래 그림은 NSP의 학습 방법\n",
        "* 연속 문장인지, 아닌지만 판단하면 되므로 Softmax의 출력은 2개이고 3만개의 출력을 갖는 MLM에 비해 빠르게 학습\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FlyapH%2FbtqzmkrVtki%2FUUqjexLh7Lt4ZwMVpjIBJ1%2Fimg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDPn-eM-Eycz"
      },
      "source": [
        "## 학습된 언어모델 전이학습(Transfer Learning)\n",
        "\n",
        "* 파인 튜닝은 학습된 언어 모델을 이용하여 실제 자연어처리 문제를 푸는 과정\n",
        "* 실질적으로 성능이 관찰되는 것은 전이학습 이지만, 언어 모델이 제대로 학습되야 전이학습 시 좋은 성능이 나옴\n",
        "* 기존 알고리즘들은 자연어의 다양한 Task에 각각의 알고리즘을 독립적으로 만들어야 했지만, BERT 개발 이후 많은 자연어처리 연구자들은 언어 모델을 만드는데 더 공을 들이게 됨\n",
        "* 전이학습 Task의 성능도 훨씬 더 좋아짐\n",
        "* 전이학습은 라벨이 주어지므로 지도학습(Supervised learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyfDPHbkFHgr"
      },
      "source": [
        "* 전이학습은 BERT의 언어 모델의 출력에 추가적인 모델을 쌓아서 사용\n",
        "* 일반적으로 복잡한 CNN, LSTM, Attention을 쌓지 않고 간단한 DNN만 쌓아도 성능이 잘 나오며 별 차이가 없다고 알려짐\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdHqgat%2Fbtqzl4CSqNd%2F7q3g5hxTcAENvvcu1wK6KK%2Fimg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i05t2LQaFjVS"
      },
      "source": [
        "## BERT 친구들\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbMaiOM%2FbtqznO6UO3m%2FwvMAVAZDLngmplVbkn0gqK%2Fimg.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YNQLQHji8lM"
      },
      "source": [
        "# BERT 네이버 영화 리뷰 분류\n",
        "\n",
        "* 참고: https://github.com/NLP-kr/tensorflow-ml-nlp-tf2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvIRqh0AIl8d"
      },
      "source": [
        "## 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiKkbJmvkd98"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAd2vXBshrRh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDRKt69ChrRY"
      },
      "source": [
        "## 데이터 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP74lUeSIABN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OboRn3Vcj_GW"
      },
      "source": [
        "* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
        "* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_eD3MbDphrSB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_ZCDWgskiRp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVnAFFU-kiny"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbG9rFUZkoXv"
      },
      "source": [
        "## BertTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYk5cINxlIcM"
      },
      "source": [
        "* 참조: https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ymur-MI3hrSJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tagwY491hrSO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQJguadKhrSS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaO33a6ChrSW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfubbrJBxhDa"
      },
      "source": [
        "## 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df3MZ75XhrSa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvBygAVGhrSf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzUB0CmvhrSh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2C2pRPOxzqm"
      },
      "source": [
        "## 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DB2-EymhrSp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLu7M6TohrSt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}